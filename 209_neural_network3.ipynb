{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V09Yxe9Z1kF"
      },
      "source": [
        "# 신경망 작동법 이해하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lherMp4VGZy"
      },
      "source": [
        "## 신경망의 학습은 어떻게 이루어질까요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e49EFqbdViZD"
      },
      "source": [
        "아래 이미지를 보면서 신경망이 어떻게 학습을 진행하는지 상상해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8QmGkNQdgD"
      },
      "source": [
        "<img src=\"https://i.imgur.com/dlGareT.gif\" alt=\"backpropagation\" width=600>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjZ4HIqeOa9G"
      },
      "source": [
        "- **위 그림에서 설명하고 있는 과정은 다음과 같습니다.**\n",
        "\n",
        "1. 데이터가 입력되면 신경망 각 층에서 **가중치 및 활성화 함수 연산**을 반복적으로 수행합니다.\n",
        "2. 1의 과정을 모든 층에서 반복한 후에 **출력층에서 계산된 값을 출력**합니다.\n",
        "3. **손실 함수**를 사용하여 **예측값(Prediction)과 실제값(Target)의 차이**를 계산합니다.\n",
        "4. **경사하강법과 역전파**를 통해서 **각 가중치를 갱신**합니다.\n",
        "5. 학습 중지 기준을 만족할 때까지 **1-4의 과정을 반복**합니다.\n",
        "\n",
        "1-4의 과정을 **Iteration(이터레이션)**이라고 하며 매 Iteration 마다 가중치가 갱신됩니다.<br/>\n",
        "Iteration 은 **<font color=\"ff6f61\">순전파(1&2), 손실 계산(3), 역전파(4)</font>**로 나눠볼 수 있는데요.<br/>\n",
        "먼저 비유를 통해서 신경망 학습에 대해 알아보고 각 과정에 대해서 하나씩 알아보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xlGGjSJRSP"
      },
      "source": [
        "### 순전파(Forward Propagation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNeHKC89vGUX"
      },
      "source": [
        "위에서 알아본 것처럼 신경망은 학습 과정에서 첫 번째로 **<font color=\"ff6f61\">순전파(Forward propagation)</font>**를 수행합니다.<br/>\n",
        "순전파는 **입력층에서 입력된 신호가 은닉층의 연산을 거쳐 출력층에서 값을 내보내는 과정**인데요.\n",
        "\n",
        "위 이미지에서 왼쪽에서 오른쪽으로 신호가 전달되는 과정을 순전파라고 하며<br/>\n",
        "각 층에서의 연산 과정은 다음과 같습니다.\n",
        "\n",
        "1. 입력층(혹은 이전 은닉층)으로부터 신호를 전달받습니다.\n",
        "2. 입력된 데이터에 **가중치-편향 연산**을 수행합니다.\n",
        "3. 가중합을 통해 구해진 값은 **활성화 함수**를 통해 다음 층으로 전달됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTXE6ix3dk51"
      },
      "source": [
        "### 손실 함수(Loss function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1pPdhnaIjKA"
      },
      "source": [
        "신경망은 손실 함수를 최소화 하는 방향으로 가중치를 갱신합니다.<br/>\n",
        "그렇기 때문에 손실 함수를 잘 정의해주어야 가중치가 제대로 갱신될 수 있겠죠?\n",
        "\n",
        "입력 데이터를 신경망에 넣어 순전파를 거치면 마지막에는 출력층을 통과한 값이 도출됩니다.<br/>\n",
        "이 때 출력된 값과 그 데이터의 타겟값을 **손실 함수에 넣어 손실(Loss or Error)를 계산**하게 됩니다.\n",
        "\n",
        "대표적인 손실 함수로는 여러분이 머신러닝 Section 에서 배웠던<br/>\n",
        "**MSE(Mean-Squared Error), CEE(Cross-Entropy Error)** 등이 있습니다.\n",
        "\n",
        "일반적으로 회귀의 손실 함수로는 **MSE** 혹은 **MAE**를,<br/>\n",
        "이진 분류의 손실 함수로는 **binary_crossentropy**를,<br/>\n",
        "다중 분류의 손실 함수로는 **categorical_crossentropy**와 **sparse_categorical_crossentropy**를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMi8h8egQrrJ"
      },
      "source": [
        "### 역전파(Backward Propagation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JEOjFrHQaOx"
      },
      "source": [
        "**<font color=\"ff6f61\">역전파(Backpropagation)</font>**는 말 그대로 순전파와는 **반대 방향으로 손실(Loss or Error) 정보를 전달**해주는 과정입니다.\n",
        "\n",
        "순전파가 **입력 신호 정보를 입력층부터 출력층까지 전달하여 값을 출력**하는 알고리즘이었다면,<br/>\n",
        "역전파는 구해진 **손실 정보를 출력층부터 입력층까지 전달하여 각 가중치를 얼마나 업데이트 해야할 지를 구하는** 알고리즘입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8znnSpvAQras"
      },
      "source": [
        "신경망은 매 반복마다 **손실(Loss)을 줄이는 방향**으로 가중치를 업데이트합니다.<br/>\n",
        "그렇다면 **손실을 줄이기 위해서** 어떻게 가중치를 수정해야 할까요?\n",
        "\n",
        "가중치 수정 방향을 결정하는 것이 바로 **<font color=\"ff6f61\">경사 하강법(Gradient Descent, GD)</font>**입니다.<br/>\n",
        "경사 하강법에 대해 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aguCH12CVGZ0"
      },
      "source": [
        "## 경사 하강법(Gradient Descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dhx_j25GyaE"
      },
      "source": [
        "아래 그림을 보면 알 수 있듯, **손실 함수 $J$ 의 경사(Gradient)가 작아지는 방향으로 업데이트** 하면 손실 함수의 값을 줄일 수 있습니다.<br/>\n",
        "매 Iteration 마다 **<font color=\"ff6f61\">해당 가중치에서의 비용 함수의 도함수(=비용 함수를 미분한 함수)를 계산</font>하여** 경사가 작아질 수 있도록 가중치를 변경합니다.\n",
        "\n",
        "$i$ 번째 가중치인 $\\theta_i$ 가 갱신되는 모습을 수식으로는 다음과 같이 나타낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcjpXc7oU91t"
      },
      "source": [
        "<img src=\"https://i.imgur.com/ic91umJ.png\" height=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81XORsbhVEOT"
      },
      "source": [
        "그림을 보면서 기울기가 양수(+)일 때에는 왜 왼쪽(-)으로 이동하게 되는지,<br/>\n",
        "기울기가 음수(-)일 때에는 왜 오른쪽(+)으로 이동하게 되는지 생각해봅시다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV6-h0j5XuAC"
      },
      "source": [
        "<img src=\"https://i.imgur.com/ehYYRtw.png\" height=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TDaCMe8YDK"
      },
      "source": [
        "예를 들면, 아래와 같이 가중치가 변하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJV0fh1s8K3v"
      },
      "source": [
        "<img src=\"https://i.imgur.com/ostAP3w.gif\" height=\"300\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_nVOZ6amwdZ"
      },
      "source": [
        "### 다시, 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC0ZoW62mzvh"
      },
      "source": [
        "그러면 각각의 가중치는 어떻게 갱신될까요?\n",
        "\n",
        "이 과정에서 역전파의 주요 메커니즘인 **편미분**과 **Chain rule(연쇄 법칙)**이 사용됩니다.<br/>\n",
        "위 식에서 볼 수 있었던 것처럼 특정 가중치 $(\\theta_i)$ 에 대한 기울기는\n",
        "아래 식과 같이 손실 함수를 해당 가중치로 **편미분**하여 구할 수 있습니다.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta_i} J(\\theta)\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "그렇다면 모든 가중치에 대한 값은 어떻게 구할 수 있을까요?<br/>\n",
        "여기서 바로 **Chain rule**이 적용됩니다.<br/>\n",
        "연쇄 법칙이란 아래 식과 같이 특정 변수에 대한 (편)미분 값을 다른 변수의 미분을 사용하여 나타낼 수 있는 방식입니다.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_i} = \\frac{\\partial J(\\theta)}{\\partial \\theta_x} \\cdot \\frac{\\partial \\theta_x}{\\partial \\theta_i} = \\frac{\\partial J(\\theta)}{\\partial \\theta_x} \\cdot \\frac{\\partial \\theta_x}{\\partial \\theta_y} \\cdot \\frac{\\partial \\theta_y}{\\partial \\theta_i}\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "연쇄 법칙을 사용하여 각 변수가 얼마나 수정되어야 할 지에 대한 정보를 전달할 수 있게 됩니다.\n",
        "\n",
        "> ❗️ ***역전파 메커니즘에 대한 수학적인 설명은 Reference를 참조해주세요 !***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obSHWEWzyKUV"
      },
      "source": [
        "### 옵티마이저(Optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEpaTmSiSB8K"
      },
      "source": [
        "다음은 **<font color=\"ff6f61\">옵티마이저(Optimizer)</font>** 입니다.\n",
        "\n",
        "옵티마이저는 쉽게 말해 **<font color=\"ff6f61\">경사를 내려가는 방법을 결정</font>**하는데요.<br/>\n",
        "대표적인 옵티마이저로는 아래와 같은 것들이 있습니다. (다 외우지 않아도 됩니다!)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwgmiQntR_7R"
      },
      "source": [
        "<img src=\"https://i.imgur.com/UQfpjpP.png\" height=\"350\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIIVMCJ4ffD8"
      },
      "source": [
        "일반적인 경사 하강법(GD)에서는 모든 입력 데이터에 대한 손실 함수의 기울기를 계산한 후에 가중치를 업데이트 하였습니다.<br/>\n",
        "즉, Iteration 마다 모든 데이터를 다 사용하게 되는 것이죠.<br/>\n",
        "입력 데이터가 적다면 이 방법으로도 빠르게 가중치를 금방 갱신할 수 있습니다.\n",
        "\n",
        "하지만 실제로는 이보다 훨씬 더 큰 데이터를 다루게 되는데요.<br/>\n",
        "만약 입력 데이터가 수천만개라면 모든 데이터에 대해 손실을 계산하는 과정이 굉장히 오래 걸리게 됩니다.<br/>\n",
        "그러면 가중치를 수정하는데 굉장히 오랜 시간이 들어가겠죠?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW5oQ-V1R6N-"
      },
      "source": [
        "#### 확률적 경사 하강법(Stochastic Gradient Descent, SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afuCVMEiYj_g"
      },
      "source": [
        "그래서 등장한 것이 바로 **<font color=\"ff6f61\">확률적 경사 하강법</font>**과 **<font color=\"ff6f61\">미니 배치(Mini-batch) 경사 하강법</font>**입니다.\n",
        "\n",
        "확률적 경사 하강법(SGD)은 전체 데이터에서 **하나의 데이터**를 뽑아서 신경망에 입력한 후 손실을 계산합니다.<br/>\n",
        "그리고 그 손실 정보를 역전파하여 신경망의 가중치를 업데이트하게 됩니다.<br/>\n",
        "다시 말하면, Iteration 마다 1개의 데이터만을 사용하는데요.\n",
        "\n",
        "그렇기 때문에 **가중치를 빠르게 업데이트** 할 수 있다는 장점이 있습니다.<br/>\n",
        "물론 확률적 경사 하강법에도 단점이 있습니다. 1개의 데이터만 보기 때문에 학습 과정에서 불안정한 경사 하강을 보인다는 점인데요.\n",
        "\n",
        "아래 그림에서 확률적 경사 하강법(왼쪽)과 일반적인 경사 하강법(오른쪽)에서 경사 하강이 어떻게 일어나는 지의 차이를 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctEjlSZ1Ympr"
      },
      "source": [
        "그래서 두 방법을 적절히 융화한 **미니 배치(Mini-batch) 경사 하강법**이 등장하게 되었습니다.<br/>\n",
        "N개의 데이터로 미니 배치를 구성하여 해당 미니 배치를 신경망에 입력한 후 이 결과를 바탕으로 가중치를 업데이트합니다.<br/>\n",
        "즉, Iteration 마다 N개(=배치 사이즈)의 데이터를 사용하게 됩니다.\n",
        "\n",
        "일반적으로는 두 방법의 장점을 적절히 융화한 미니 배치 경사 하강법을 많이 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiIjO1tuCHCK"
      },
      "source": [
        "- **배치 사이즈(Batch Size)**\n",
        "\n",
        "미니 배치 경사 하강법에서 사용하는 미니 배치의 크기를 **배치 사이즈(Batch size)** 라고 합니다.<br/>\n",
        "일반적으로 배치 사이즈는 2의 배수로 설정하며, 메모리 크기가 허락한다면 큰 배치 사이즈를 쓰는 것이 학습을 안정적으로 진행할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCYNZGruYq88"
      },
      "source": [
        "- **여러 가지 옵티마이저(Optimizer)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krP5bCmNMtWB"
      },
      "source": [
        "<img src=\"https://i.imgur.com/DYoGuTT.gif\" height=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xw-3W2kYzrb"
      },
      "source": [
        "여러 가지 옵티마이저 중에서 어떤 것이 가장 좋다고 말하기는 어렵습니다.<br/>\n",
        "문제마다, 데이터마다 달라지기 때문에 여러 옵티마이저를 적용하면서 서로 비교해보아야 하는데요.\n",
        "\n",
        "다음 강의에서 최적의 하이퍼파라미터를 찾아보면서 여러 옵티마이저를 비교해 볼 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLgxCdK5v_6e"
      },
      "source": [
        "## Tensorflow 신경망 예제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziN9Vc4QQCXz"
      },
      "source": [
        "이번 시간에는 또 다른 예제 데이터인 Fashion MNIST 예제를 신경망으로 풀어보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL3Sog6xktHL"
      },
      "source": [
        "### Fashion MNIST 예제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJEAiXAz88hM"
      },
      "source": [
        "> ❓ ***그렇다면 MNIST 예제는 이진 분류, 다중 분류, 회귀 중 어디에 속할까요? <br/>\n",
        "<font color=\"ff6f61\">항상 문제를 풀기 전에 자신이 풀고자 하는 문제가 어디에 속하는 지 생각</font>해보도록 합시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUjBSQtB9Fo_"
      },
      "source": [
        "1. **먼저 필요한 패키지와 라이브러리를 불러옵니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jVtVqIp9JtE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glc0ohHs9IPw"
      },
      "source": [
        "2. **데이터셋을 불러온 후 학습 데이터셋(Train Dataset)과 시험 데이터셋(Test Dataset)으로 나누어(Split)주고 픽셀값을 정규화 하여줍니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua6p5z159YL9",
        "outputId": "3a7281e1-f27a-400c-d5ff-0b374d59653e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHuX2beXS929",
        "outputId": "ae41bbe0-5ad5-4683-ac08-232795736103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMmF-JWM9msg"
      },
      "source": [
        "이미지 데이터에서는 정규화하는 과정이 중요합니다. 빼먹지 않도록 주의해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNL0nv5L9jtm"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "AV-FRPkMTMO1",
        "outputId": "5d8251e2-07a2-4a36-92e3-e97d50468f8f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "\n",
        "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzJ9Wejf9Xst"
      },
      "source": [
        "3. **레이블이 어떻게 구성되어 있는 지 확인해봅니다.**\n",
        "\n",
        "    데이터의 레이블 구성 형태를 살펴봅니다.<br/>\n",
        "    처음보는 데이터의 경우 데이터 자체를 디스플레이 하여 보면 도움이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOKNf_559eye",
        "outputId": "8bef3d7f-b3eb-4989-ac44-93b48e8a9093"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCCLTvqu98bE"
      },
      "source": [
        "4. **이제 본격적으로 신경망 모델을 구축해보겠습니다.**\n",
        "\n",
        "> ❗️ ***아래 코드에서 출력층의 노드 수는 몇 개인지, 출력층의 활성화 함수는 무엇인지, 손실 함수는 어떻게 지정하였는지에 주목해봅시다.***<br/>\n",
        "❗️ ***`.summary()`를 활용하면 모델의 구조를 빠르게 파악해 볼 수 있습니다.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0LVdBzF98J3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysFo2tw0TakE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce49C91l-TuM",
        "outputId": "a5c4fa1c-c103-4a33-fed4-f5f2cfd604da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7850 (30.66 KB)\n",
            "Trainable params: 7850 (30.66 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXkvdWo-T23F"
      },
      "source": [
        "> ❗️ ***위 모델의 파라미터 수는 총 7,850개입니다. 왜 7,850개가 될 지에 대해 생각해봅시다.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_J92keOlIDf",
        "outputId": "ca1adfce-3551-4764-e449-b7040120784b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 3ms/step - loss: 0.5980 - accuracy: 0.7984\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4619 - accuracy: 0.8429\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4352 - accuracy: 0.8504\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4235 - accuracy: 0.8543\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4135 - accuracy: 0.8572\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4074 - accuracy: 0.8583\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4027 - accuracy: 0.8591\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3976 - accuracy: 0.8620\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3944 - accuracy: 0.8632\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3920 - accuracy: 0.8630\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b9e70372c20>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knPbosgK-xtc"
      },
      "source": [
        "5. **학습한 신경망 모델을 사용하여 평가합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90hFOU1XBA82",
        "outputId": "de4f128e-07e0-41fa-bd99-010e9a6becae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 - 1s - loss: 0.4527 - accuracy: 0.8429 - 659ms/epoch - 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.45274117588996887, 0.8428999781608582]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqd1TOUv6102"
      },
      "source": [
        "### 추가 고려 사항\n",
        "\n",
        "- **학습률(Learning rate)**\n",
        "\n",
        "    **<font color=\"ff6f61\">학습률(Learning rate, `lr`)</font>**이란 매 가중치에 대해 구해진 기울기 값을 얼마나 경사 하강법에 적용할 지를 결정하는 하이퍼파라미터입니다.  \n",
        "    지난 시간에 보았던 경사 하강법 수식에서 학습률이 어디에 위치하는지 확인해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AVX33ZINayy"
      },
      "source": [
        "<img src=\"https://i.imgur.com/ic91umJ.png\" height=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCZyAvXYNeBl"
      },
      "source": [
        "위 식에서 볼 수 있는 것처럼 해당 지점에서의 기울기를 구하여 기울기가 감소하는$(-)$ 방향으로 이동하게 되는데요.<br/>\n",
        "학습률은 **얼마나 이동할 지를 조정하는 하이퍼파라미터**입니다.\n",
        "\n",
        "경사 하강법이 산길을 내려가는 과정이라면 학습률은 **보폭을 결정**하게 됩니다.<br/>\n",
        "학습률이 크면 보폭이 크니 Iteration 마다 성큼성큼 이동하고, 작으면 보폭이 작아 조금씩만 이동하게 됩니다.<br/>\n",
        "그렇다면 학습률을 잘못 설정하면 어떻게 될까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbBjzju8N1zo"
      },
      "source": [
        "- **학습률이 너무 크거나 작으면 어떻게 될까요?**\n",
        "\n",
        "> ❗️ *아래는 학습률이 너무 클 때와 작을 때의 경사하강법을 나타낸 그림입니다.*  \n",
        "> *그림을 기억하면서 최적의 학습률이 왜 중요한 지에 대해 생각해봅시다.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JJV693y7fUC"
      },
      "source": [
        "<img src=\"https://i.imgur.com/RfBFgKs.png\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-hKd9jMK6xE"
      },
      "source": [
        "그림에서 확인할 수 있는 것처럼<br/>\n",
        "**학습률이 너무 낮으면 최적점에 이르기까지 너무 오래 걸리거나, 주어진 Iteration 내에서 최적점에 도달하는 데 실패**하기도 합니다.<br/>\n",
        "반대로 **너무 높으면 경사 하강 과정에서 발산하면서 모델이 최적값을 찾을 수 없게** 되어버립니다.\n",
        "\n",
        "그렇기 때문에 최적의 학습률을 찾는 것은 학습에서 중요한 요소인데요.<br/>\n",
        "위와 같은 문제를 해결하기 위해서 사용되는 방법이 **학습률 감소/계획법**입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONqTmzH6cBTo"
      },
      "source": [
        "##### 1) **학습률 감소(Learning rate Decay)**\n",
        "\n",
        "학습률 감소는 Adagrad, RMSprop, Adam 과 같은 주요 옵티마이저에 이미 구현되어 있기 때문에 쉽게 적용할 수 있습니다.  \n",
        "해당 옵티마티저의 하이퍼파라미터를 조정하면 감소 정도를 변화시킬 수 있습니다.\n",
        "\n",
        "아래와 같이 **`.compile`** 내에 있는 **`optimizer=`** 에 Adam 등의 옵티마이저 적용 후 내부 하이퍼파라미터를 변경하면 학습률 감소를 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjwyqm0xAwAA",
        "outputId": "48579d49-fc47-4d4f-b042-8bde9c2aaedc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "# optimizer 내 lr(learning rate) 인자를 통해 학습률을 설정할 수 있습니다. beta_1 인자는 학습률 감소율을 설정하며 Adam 내 수식의 변수를 그대로 사용합니다.\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1 = 0.89)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SQsF4emxP1-"
      },
      "source": [
        " * Dropout (드롭아웃)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWndbcDy20UL"
      },
      "source": [
        "**Dropout(드롭아웃)**은 Iteration 마다 **레이어 노드 중 일부를 사용하지 않으면서 학습을 진행하는 방법**입니다.<br/>\n",
        "매 번 다른 노드가 학습되면서 전체 가중치가 과적합되는 것을 방지할 수 있습니다.<br/>\n",
        "아래는 Dropout을 적용했을 때의 신경망에서 학습되는 노드를 나타낸 그림입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5M7ArjZG-lZ"
      },
      "source": [
        "<img src=\"https://i.imgur.com/rAyIZxV.png\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnxtUB2kHHqO"
      },
      "source": [
        "Dropout 을 적용할 때에는 0~1 사이의 실수를 입력하게 되는데요.<br/>\n",
        "모델 내에 있는 특정 레이어의 노드 연결을 지정해 준 비율만큼 강제로 끊습니다.<br/>\n",
        "매 Iteration 마다 랜덤하게 노드를 차단하여 다른 가중치를 학습하도록 조정하기 때문에 과적합을 방지할 수 있게 됩니다.\n",
        "\n",
        "Keras 에서는 아래와 같이 Dropout 을 적용하고 싶은 층 다음에 `Dropout` 함수를 추가하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoloPCLn-aN3"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras import regularizers\n",
        "# from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Dense(64,\n",
        "#       kernel_regularizer=regularizers.l2(0.01),\n",
        "#       activity_regularizer=regularizers.l1(0.01))\n",
        "# Dropout(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeq9Fkhm3BlY"
      },
      "source": [
        "### Fashion MNIST 신경망 예제에 기법 적용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf415x9HeuHJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-shEO-j7YVC"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuHlAVpS4ix1"
      },
      "source": [
        "1) **데이터셋을 불러옵니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXzVrPl3jZSL",
        "outputId": "0e29b28a-26f3-4513-f8a7-47b5c149ccef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ETLEjs4k5w"
      },
      "source": [
        "2) **데이터를 정규화(Normalization)합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGkSj-XVjdOP"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255.\n",
        "X_test = X_test / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TORaDD1U4uMI"
      },
      "source": [
        "3) **레이블의 개수와 형태를 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF-QSeSLjdp0",
        "outputId": "6763f543-b1b3-465f-e605-087bfea0eea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCC0Z2ZH49ih"
      },
      "source": [
        "4) **신경망 모델을 구축하고 Compile 합니다.**\n",
        "\n",
        "  구축 과정에서 위에서 학습하였던 **Weight Decay(가중치 감소), Dropout(드롭아웃)**을 적용해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZBg5E4ZjSrQ"
      },
      "outputs": [],
      "source": [
        "# 기본적인 신경망을 만드는 코드\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(64,\n",
        "          kernel_regularizer=regularizers.l2(0.01),\n",
        "          activity_regularizer=regularizers.l1(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzSIlxcx5SBG"
      },
      "source": [
        "Compile 설정에서 위에서 학습하였던 **Learning rate Decay(학습률 감소)**를 적용해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkJM3eSR3ue9"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1 = 0.89)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFqWpUI1jmrS",
        "outputId": "f6a1ab6b-fe6d-4ffe-a607-4cb81a8b77f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                50240     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50890 (198.79 KB)\n",
            "Trainable params: 50890 (198.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpno3z7S5ba1"
      },
      "source": [
        "5) **신경망 모델을 학습합니다.**\n",
        "\n",
        "먼저 학습 과정에서 **Early Stopping(조기 종료)**를 적용할 수 있도록<br/>\n",
        "파라미터 저장 경로와 조기 종료 옵션을 설정하여 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMA8PURlAK-7"
      },
      "outputs": [],
      "source": [
        "# 파라미터 저장 경로를 설정하는 코드입니다.\n",
        "\n",
        "checkpoint_filepath = \"FMbest.hdf5\" # .hdf5는 데이터를 저장하는 확장자입니다. 'FMbest.hdf5' 파일로 최고 성능의 모델을 저장하며 .h5와 동일한 확장자입니다.\n",
        "\n",
        "# moitor : 개선을 확인할 지표\n",
        "# min_delta : 개선이 있다고 판단되는 최소 변경값. min_delta보다 변경 사항이 작다면 개선이 없다고 판단(default=0)\n",
        "# patience : 개선이 없을 경우 개선되기까지 기다리는 epochs의 수\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPXb3ALLARPA"
      },
      "outputs": [],
      "source": [
        "# ModelCheckpoint를 통해 모델을 저장합니다.\n",
        "\n",
        "# filepath : 파일이 저장될 경로. 앞서 설정한 것처럼 'FMbest.hdf5'에 저장됩니다.\n",
        "# save_best_only : 최고 성능을 보이는 경우만 저장합니다. False일 경우 filepath에 모든 학습 과정이 저장됩니다.\n",
        "# save_weights_only : 모델의 가중치만 저장합니다. False일 경우 레이어까지 모두 저장합니다.\n",
        "# mode : 검증 지표가 val_acc(정확도)일 경우 높을 수록 좋기 때문에 'max'로 설정, val_loss일 경우 낮을 수록 좋기 때문에 'min'으로 설정, 'auto'의 경우 자동으로 탐지하여 진행함.\n",
        "# save_freq : 매 에폭마다 저장을 시도합니다. integer로 설정할 경우 설정한 수만큼의 iteration마다 모델을 저장합니다.\n",
        "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "    save_weights_only=True, mode='auto', save_freq='epoch', options=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puMqk6o8AVjn",
        "outputId": "da87de69-cce9-400b-d6ae-39fe7f7dcf86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 1.2253 - accuracy: 0.7691\n",
            "Epoch 1: val_loss improved from inf to 0.88819, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 11s 4ms/step - loss: 1.2247 - accuracy: 0.7692 - val_loss: 0.8882 - val_accuracy: 0.7978\n",
            "Epoch 2/30\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.7906\n",
            "Epoch 2: val_loss did not improve from 0.88819\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.9278 - accuracy: 0.7905 - val_loss: 0.9174 - val_accuracy: 0.7915\n",
            "Epoch 3/30\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.9087 - accuracy: 0.7899\n",
            "Epoch 3: val_loss did not improve from 0.88819\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9096 - accuracy: 0.7897 - val_loss: 0.9150 - val_accuracy: 0.7870\n",
            "Epoch 4/30\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.9001 - accuracy: 0.7929\n",
            "Epoch 4: val_loss improved from 0.88819 to 0.84114, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8999 - accuracy: 0.7931 - val_loss: 0.8411 - val_accuracy: 0.8085\n",
            "Epoch 5/30\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.8943 - accuracy: 0.7915\n",
            "Epoch 5: val_loss improved from 0.84114 to 0.79882, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8947 - accuracy: 0.7913 - val_loss: 0.7988 - val_accuracy: 0.8138\n",
            "Epoch 6/30\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.8858 - accuracy: 0.7925\n",
            "Epoch 6: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8857 - accuracy: 0.7926 - val_loss: 0.8312 - val_accuracy: 0.8067\n",
            "Epoch 7/30\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.8866 - accuracy: 0.7931\n",
            "Epoch 7: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8865 - accuracy: 0.7931 - val_loss: 0.8795 - val_accuracy: 0.7903\n",
            "Epoch 8/30\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.8851 - accuracy: 0.7943\n",
            "Epoch 8: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.8850 - accuracy: 0.7942 - val_loss: 0.8096 - val_accuracy: 0.8155\n",
            "Epoch 9/30\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.8861 - accuracy: 0.7920\n",
            "Epoch 9: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8861 - accuracy: 0.7920 - val_loss: 0.9190 - val_accuracy: 0.7867\n",
            "Epoch 10/30\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.8838 - accuracy: 0.7924\n",
            "Epoch 10: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.8840 - accuracy: 0.7924 - val_loss: 0.7994 - val_accuracy: 0.8163\n",
            "Epoch 11/30\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.8870 - accuracy: 0.7905\n",
            "Epoch 11: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8868 - accuracy: 0.7904 - val_loss: 0.8389 - val_accuracy: 0.7961\n",
            "Epoch 12/30\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.8857 - accuracy: 0.7927\n",
            "Epoch 12: val_loss did not improve from 0.79882\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.8856 - accuracy: 0.7927 - val_loss: 0.8559 - val_accuracy: 0.7996\n",
            "Epoch 13/30\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.8871 - accuracy: 0.7920\n",
            "Epoch 13: val_loss improved from 0.79882 to 0.79219, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8866 - accuracy: 0.7922 - val_loss: 0.7922 - val_accuracy: 0.8170\n",
            "Epoch 14/30\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.8773 - accuracy: 0.7946\n",
            "Epoch 14: val_loss did not improve from 0.79219\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.8774 - accuracy: 0.7945 - val_loss: 0.8036 - val_accuracy: 0.8091\n",
            "Epoch 15/30\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.8786 - accuracy: 0.7948\n",
            "Epoch 15: val_loss did not improve from 0.79219\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8793 - accuracy: 0.7946 - val_loss: 0.8545 - val_accuracy: 0.8114\n",
            "Epoch 16/30\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.8896 - accuracy: 0.7917\n",
            "Epoch 16: val_loss did not improve from 0.79219\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.8895 - accuracy: 0.7918 - val_loss: 0.8077 - val_accuracy: 0.8134\n",
            "Epoch 17/30\n",
            "1857/1875 [============================>.] - ETA: 0s - loss: 0.8783 - accuracy: 0.7947\n",
            "Epoch 17: val_loss did not improve from 0.79219\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8782 - accuracy: 0.7948 - val_loss: 0.7974 - val_accuracy: 0.8162\n",
            "Epoch 18/30\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.8793 - accuracy: 0.7931\n",
            "Epoch 18: val_loss improved from 0.79219 to 0.78823, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.8796 - accuracy: 0.7930 - val_loss: 0.7882 - val_accuracy: 0.8189\n",
            "Epoch 19/30\n",
            "1858/1875 [============================>.] - ETA: 0s - loss: 0.8776 - accuracy: 0.7957\n",
            "Epoch 19: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8775 - accuracy: 0.7958 - val_loss: 0.8146 - val_accuracy: 0.8120\n",
            "Epoch 20/30\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.8828 - accuracy: 0.7923\n",
            "Epoch 20: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.8829 - accuracy: 0.7922 - val_loss: 0.8057 - val_accuracy: 0.8144\n",
            "Epoch 21/30\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.8778 - accuracy: 0.7950\n",
            "Epoch 21: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8780 - accuracy: 0.7950 - val_loss: 0.8369 - val_accuracy: 0.7980\n",
            "Epoch 22/30\n",
            "1860/1875 [============================>.] - ETA: 0s - loss: 0.8709 - accuracy: 0.7949\n",
            "Epoch 22: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8710 - accuracy: 0.7949 - val_loss: 0.7926 - val_accuracy: 0.8158\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.8869 - accuracy: 0.7919\n",
            "Epoch 23: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8869 - accuracy: 0.7919 - val_loss: 0.8167 - val_accuracy: 0.8157\n",
            "Epoch 24/30\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.8698 - accuracy: 0.7967\n",
            "Epoch 24: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.8700 - accuracy: 0.7966 - val_loss: 0.8304 - val_accuracy: 0.8051\n",
            "Epoch 25/30\n",
            "1860/1875 [============================>.] - ETA: 0s - loss: 0.8816 - accuracy: 0.7940\n",
            "Epoch 25: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8819 - accuracy: 0.7939 - val_loss: 0.8289 - val_accuracy: 0.8096\n",
            "Epoch 26/30\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.8762 - accuracy: 0.7938\n",
            "Epoch 26: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8762 - accuracy: 0.7938 - val_loss: 0.9331 - val_accuracy: 0.7886\n",
            "Epoch 27/30\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.8844 - accuracy: 0.7913\n",
            "Epoch 27: val_loss did not improve from 0.78823\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8838 - accuracy: 0.7916 - val_loss: 0.8222 - val_accuracy: 0.8070\n",
            "Epoch 28/30\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.8749 - accuracy: 0.7944\n",
            "Epoch 28: val_loss improved from 0.78823 to 0.78392, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8747 - accuracy: 0.7945 - val_loss: 0.7839 - val_accuracy: 0.8217\n",
            "Epoch 29/30\n",
            "1858/1875 [============================>.] - ETA: 0s - loss: 0.8808 - accuracy: 0.7940\n",
            "Epoch 29: val_loss did not improve from 0.78392\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8808 - accuracy: 0.7941 - val_loss: 0.7955 - val_accuracy: 0.8202\n",
            "Epoch 30/30\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.8824 - accuracy: 0.7933\n",
            "Epoch 30: val_loss did not improve from 0.78392\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8826 - accuracy: 0.7932 - val_loss: 0.7972 - val_accuracy: 0.8203\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b9e5c55b8e0>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=30, verbose=1,\n",
        "          validation_data=(X_test,y_test), # 검증 데이터\n",
        "          callbacks=[early_stop, save_best]) # 앞서 선언한 Early Stopping과 Model Checkpoint를 callbacks를 통해 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMFmcflC03L7"
      },
      "source": [
        "6) **조기종료 직전의 모델을 사용하여 평가를 진행합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0uTr8Gd1UqF",
        "outputId": "3340069e-ad96-47a2-cccb-4dce64135931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 155ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[4.2094078e-05, 2.8737684e-05, 1.1058110e-04, 1.2665593e-04,\n",
              "        1.2302694e-04, 8.6323008e-02, 1.4196172e-04, 1.7086932e-01,\n",
              "        1.1637054e-02, 7.3059756e-01]], dtype=float32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(X_test[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTfCeDeOlDt9",
        "outputId": "24280837-cb90-4229-ed95-f27113688246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 - 1s - loss: 0.7972 - accuracy: 0.8203 - 782ms/epoch - 2ms/step\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T05UQlnPmufW"
      },
      "source": [
        "7) **콜백(Callback)에 의해 Best 모델의 파라미터가 제대로 저장되었는지 확인하고 해당 모델로 평가를 진행합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLPMWcTCmshO",
        "outputId": "6a4677b5-f51d-4cb7-dbe5-6a597117e1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FMbest.hdf5  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls # 현제 경로를 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqg297g_1Ieh"
      },
      "source": [
        "저장된 경로로부터 모델의 파라미터(가중치)를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16bPb8k3myJe"
      },
      "outputs": [],
      "source": [
        "model.load_weights(checkpoint_filepath) # 'FMbest.hdf5'에 저장한 가중치를 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_5td5zI1NZL"
      },
      "source": [
        "불러온 모델을 사용하여 평가를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7KO7G4E1eFG",
        "outputId": "b3e06797-b942-4872-9a7d-48a7c6571e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2.7424971e-05, 3.4651355e-05, 4.9306887e-05, 1.1625543e-04,\n",
              "        5.8693986e-05, 1.2935276e-01, 6.6680535e-05, 1.9742100e-01,\n",
              "        1.4727977e-03, 6.7140043e-01]], dtype=float32)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(X_test[0:1]) # 테스트 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXNs4GfWm0rI",
        "outputId": "5a40b837-1f6f-45ac-97a3-1d9cdb969343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7839 - accuracy: 0.8217\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=1) # 테스트 지표 확인"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
